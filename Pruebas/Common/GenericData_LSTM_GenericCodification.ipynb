{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Propósito principal </font>\n",
    "- La idea de este librillo es preparar el entorno para realizar pruebas para cualquier DataSet a ser clasificado por cualquier codificación aplicada a LSTM\n",
    "\n",
    "## <font color=#cd0000> Leyenda </font>\n",
    "- Los apartados titulados con el código de colores HEX: `#cd0000` serán apartados que tendrán todos los librillos, en concreto, aquellos especificados en el apartado `Síntesis de los criterios usados` del trabajo.\n",
    "- Los apartados titulados con el código de colores HEX: `#2451ff` serán apartados de conclusiones propias de este librillo resultado de aplicar un estudio personalizado para cada planteamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Prerrequisitos </font>\n",
    "## <font color=#cd0000> Entorno de ejecución </font>\n",
    "- Cambiamos el directorio raíz del librillo para acceder cómodamente a las funciones de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Constantes y variables predefinidas </font>\n",
    "- TODO -> Añadir SEED a todas las particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEARTBEAT_PATH = \"data/heartbeat\"\n",
    "EPILEPSY_PATH = \"data/epilepsy\"\n",
    "SEGUIMIENTO_OCULAR_PATH = \"data/seguimiento-ocular/Data/Hospital\"\n",
    "SEGUIMIENTO_OCULAR_FOLDERS_ID = range(1, 12+1)\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Carga del Dataset </font>\n",
    "- TODO: Breve descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Change with known data\n",
    "from utils.load_data import import_epilepsy_dataset\n",
    "\n",
    "train, test = import_epilepsy_dataset(EPILEPSY_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Preprocesamiento </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Eliminación de datos inválidos y valores atípicos </font>\n",
    "- TODO: Breve descripción de qué es un dato inválido (-1's en columna, etc.)\n",
    "- Eliminaremos aquellos valores fuera de los percentiles 5 y 95.\n",
    "- TODO: Definiremos cuál será el límite de outliers permitido por serie temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Remove invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.remove_outliers(headers=train.get_derived_data_columns()['attrs'], outliers_limit=0.9)\n",
    "# test.remove_outliers(headers=test.get_derived_data_columns()['attrs']) -> Estos no los podemos alterar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remaining series\n",
    "print(\"Previous number of series: {}\".format(\n",
    "    len(pd.unique(train.original_data['id']))))\n",
    "print(\"Actual number of series: {}\".format(\n",
    "    len(pd.unique(train.derived_data['id']))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Normalización </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.codifications import standardize_data\n",
    "\n",
    "train.derived_data, test.derived_data = standardize_data(\n",
    "    train.derived_data, test.derived_data, headers=train.get_derived_data_columns()['attrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.derived_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Resoluciones a aplicar </font>\n",
    "- TODO:\n",
    "  - Si las series son rápidas (muchos cambios en poco tiempo) especificar resoluciones altas (sin modificaciones).\n",
    "  - Si las series son lentas (pocos cambios en mucho tiempo) especificar resoluciones bajas (eliminamos datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series lentas\n",
    "train.reduce_sampling_rate(remove_one_each_n_samples=2)\n",
    "test.reduce_sampling_rate(remove_one_each_n_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.derived_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Codificación </font>\n",
    "- TODO: Breve descripción de la codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.codifications import temporal_trend_fn\n",
    "\n",
    "train.apply_codifications([temporal_trend_fn])\n",
    "test.apply_codifications([temporal_trend_fn])\n",
    "\n",
    "train.derived_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = train.transform_derived_data_into_X_y()\n",
    "X_test, _ = test.transform_derived_data_into_X_y()\n",
    "\n",
    "y_train = train.derived_data.groupby('id').first()['class'].to_numpy()\n",
    "y_test = test.derived_data.groupby('id').first()['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Tensores entrada y salida de la red </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All lengths must be equal\n",
    "series_length = train.get_shortest_serie().shape[0]\n",
    "n_dims = len(train.get_derived_data_columns()['attrs'])\n",
    "\n",
    "(\n",
    "    train.get_shortest_serie().shape[0],\n",
    "    train.get_largest_serie().shape[0],\n",
    "    test.get_shortest_serie().shape[0],\n",
    "    test.get_largest_serie().shape[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This will determine the number of series of each split\n",
    "train_n_series = pd.unique(train.derived_data['id']).shape[0]\n",
    "test_n_series = pd.unique(test.derived_data['id']).shape[0]\n",
    "\n",
    "(train_n_series, test_n_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classifier_utils import apply_lstm_format\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "X_train, y_train = apply_lstm_format(\n",
    "    X_train, y_train, train_n_series, series_length, 2, enc)\n",
    "X_test, y_test = apply_lstm_format(\n",
    "    X_test, y_test, test_n_series, series_length, 2, enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Diseño de la topología de red </font>\n",
    "- TODO:\n",
    "  - Nº unidades LSTM (rangos a variar)\n",
    "  - Funciones de activación\n",
    "  - Tamaño por lote (batch_size)\n",
    "  - Neuronas de salida\n",
    "    - Una con activación sigmoidal si es binaria\n",
    "    - Tantas como clases haya con activación softmax + argmax para decidir qué clase será la elegida.\n",
    "      - Para hacer esto -> `k.argmax`\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "# Number of initial dimensions\n",
    "nn.add(LSTM(units=12, dropout=.2, recurrent_dropout=.2))\n",
    "# Number of Epilepsy's classes\n",
    "nn.add(Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Compilación de la red </font>\n",
    "- TODO:\n",
    "  - Optimizador -> reajuste de pesos\n",
    "    - Función de pérdida\n",
    "  - Función de pérdida\n",
    "  - Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "metrics = [\n",
    "    k.metrics.CategoricalAccuracy(name=\"ACC\"),\n",
    "    k.metrics.Precision(name='Prec'),\n",
    "    k.metrics.Recall(name='Rec'),\n",
    "    k.metrics.AUC(name='AUC')\n",
    "]\n",
    "nn.compile(optimizer=RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=metrics)\n",
    "nn.build(input_shape=X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Visualización de resultados preliminares </font>\n",
    "- TODO:\n",
    "  - Verificar sobreajuste y tomar medidas en caso de darse:\n",
    "    - Regularizadores L1L2\n",
    "    - Tasa de Dropout\n",
    "    - Decrementar épocas de entrenamiento\n",
    "  - Si tarda en converger:\n",
    "    - Inicializar correctamente los pesos (GlorotNormal, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_metrics(history):\n",
    "    for metric in history.history.keys():\n",
    "        if not metric.startswith('val_'):\n",
    "            plt.plot(history.history[metric], label=metric)\n",
    "            plt.plot(history.history[f'val_{metric}'], label=f'val_{metric}')\n",
    "            plt.title(metric)\n",
    "            plt.ylabel('')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Entrenamiento del modelo </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def argmax(x, n_classes):\n",
    "    all_predictions = []\n",
    "    for max_class_position in K.argmax(x):\n",
    "        prediction = np.zeros(n_classes)\n",
    "        prediction.put(max_class_position, 1)\n",
    "        all_predictions.append(prediction)\n",
    "    return np.asarray(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = nn.fit(X_train, y_train, epochs=epochs,\n",
    "                 validation_data=(X_test, y_test))\n",
    "nn.summary()\n",
    "print('\\n\\n')\n",
    "\n",
    "y_pred = argmax(nn.predict(X_test), 4)\n",
    "\n",
    "show_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = enc.inverse_transform(y_test)\n",
    "y_pred = enc.inverse_transform(y_pred)\n",
    "print(confusion_matrix(y_real, y_pred))\n",
    "print(classification_report(y_real, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Randomized Search </font>\n",
    "- Búsqueda de hiper-parámetros aleatoria con LSTM maximizando ``macro avg f1-score``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Randomized Search + CV </font>\n",
    "- Solo si tenemos muchos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import utils.constants as cs\n",
    "from utils.classifier_utils import (windowed_cross_val,\n",
    "                                    compute_classification_reports_means)\n",
    "from utils.plot_utils import pretty_print_classification_report_dict\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "PKL_DIR = 'pkl/LSTM/'\n",
    "\n",
    "\n",
    "def lstm_randomized_search_cv(\n",
    "        windowed_series,\n",
    "        relation_with_series,\n",
    "        prefix,\n",
    "        cv=5,\n",
    "        lstm_dict={}):\n",
    "    global PKL_DIR\n",
    "    all_clf_used = {}\n",
    "\n",
    "    n_samples = 5\n",
    "    units_list = random.sample(\n",
    "        [30, 50, 75, 100, 200], n_samples)\n",
    "    learning_rate_list = random.sample(\n",
    "        [1e-2, 1e-3, 1e-4, 1e-5, 1e-6], n_samples)\n",
    "\n",
    "    best_hyp_params = None\n",
    "    best_score = 0\n",
    "    for units in units_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            clf_used = {}\n",
    "\n",
    "            lstm_dict[cs.LSTM_HYP_PARAM_UNITS] = units\n",
    "            lstm_dict[cs.LSTM_HYP_PARAM_LEARNING_RATE] = learning_rate\n",
    "            reports = windowed_cross_val(\n",
    "                None,\n",
    "                windowed_series,\n",
    "                relation_with_series,\n",
    "                cv=cv,\n",
    "                seed=SEED,\n",
    "                drop_columns=['class'],\n",
    "                estimator_type=cs.ESTIMATOR_LSTM,\n",
    "                lstm_dict=lstm_dict\n",
    "            )\n",
    "\n",
    "            mean_report = compute_classification_reports_means(reports)\n",
    "            all_clf_used[(units, learning_rate)] = (clf_used, str(mean_report))\n",
    "\n",
    "            if mean_report['macro avg']['f1-score'] >= best_score:\n",
    "                best_score = mean_report['macro avg']['f1-score']\n",
    "                best_hyp_params = (units, learning_rate)\n",
    "                best_report = mean_report\n",
    "\n",
    "            print(\"\\t\\t--------------ACTUAL BEST: Units={}; Learning Rate={}--------------\"\n",
    "                  .format(best_hyp_params[0], best_hyp_params[1]))\n",
    "            pretty_print_classification_report_dict(best_report)\n",
    "            print(\"\\t\\t--------------ITERATION: Units={}; Learning Rate={}--------------\"\n",
    "                  .format(units, learning_rate))\n",
    "            pretty_print_classification_report_dict(mean_report)\n",
    "\n",
    "    with open(PKL_DIR + prefix, 'wb') as file:\n",
    "        pickle.dump(all_clf_used, file)\n",
    "\n",
    "    return best_hyp_params, best_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dict = {\n",
    "    cs.LSTM_SERIES_LENGTH: series_length,\n",
    "    cs.LSTM_SEQUENCES_FRAGMENTER: 2,\n",
    "    cs.LSTM_FITTED_LABELS_ENCODER: enc,\n",
    "    cs.LSTM_ARGMAX_FUNCTION: argmax,\n",
    "    cs.LSTM_N_CLASSES: 4,\n",
    "    cs.LSTM_HYP_PARAM_EPOCHS: 50\n",
    "}\n",
    "\n",
    "lstm_randomized_search_cv(\n",
    "    train.derived_data,\n",
    "    train.derived_data_windows_per_serie,\n",
    "    'lstm_sample',\n",
    "    cv=5,\n",
    "    lstm_dict=lstm_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Randomized Search con múltiples ejecuciones en lugar de Validación Cruzada </font>\n",
    "- Solo si tenemos pocos datos\n",
    "- Ejecutaremos el mismo modelo sobre diferentes particiones del conjunto de datos original para observar su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Análisis de resultados </font>\n",
    "- TODO - Un breve análisis de los resultados obtenidos para las diferentes resoluciones, ventanas, ...\n",
    "- Visualización de gráficos para determinar si se pueden obtener mejores resultados con una serie de hiper-parámetros concretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Conclusiones </font>\n",
    "- TODO - Unas breves conclusiones sobre los resultados obtenidos (influencia de la codificación, ...)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a096ff48e453ebcfb843df4ab333716150f391fd6f6f069a95d41746473af77b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tfg_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
