{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Propósito principal </font>\n",
    "- La idea de este librillo es preparar el entorno para realizar pruebas para cualquier DataSet a ser clasificado por cualquier codificación aplicada a LSTM\n",
    "\n",
    "## <font color=#cd0000> Leyenda </font>\n",
    "- Los apartados titulados con el código de colores HEX: `#cd0000` serán apartados que tendrán todos los librillos, en concreto, aquellos especificados en el apartado `Síntesis de los criterios usados` del trabajo.\n",
    "- Los apartados titulados con el código de colores HEX: `#2451ff` serán apartados de conclusiones propias de este librillo resultado de aplicar un estudio personalizado para cada planteamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Prerrequisitos </font>\n",
    "## <font color=#cd0000> Entorno de ejecución </font>\n",
    "- Cambiamos el directorio raíz del librillo para acceder cómodamente a las funciones de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../..')\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Constantes y variables predefinidas </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEARTBEAT_PATH = \"data/heartbeat\"\n",
    "EPILEPSY_PATH = \"data/epilepsy\"\n",
    "SEGUIMIENTO_OCULAR_PATH = \"data/seguimiento-ocular/Data/Hospital\"\n",
    "SEGUIMIENTO_OCULAR_FOLDERS_ID = range(1, 12+1)\n",
    "\n",
    "DATA_TO_SAVE = \"HeartBeat\"\n",
    "\n",
    "PKL_DIR = \"pkl/<classifier>/<ds>/\"\n",
    "PKL_NAME = \"<ds>_<classifier>_<codif>.pkl\"\n",
    "\n",
    "SEED = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Carga del Dataset </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Change with known data\n",
    "from utils.load_data import import_epilepsy_dataset\n",
    "\n",
    "# train, test = import_heartbeat_dataset(HEARTBEAT_PATH)\n",
    "# all_data = import_seguimiento_ocular_dataset(SEGUIMIENTO_OCULAR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump(train, open(DATA_TO_SAVE + \"_tmp_train_data.pkl\", 'wb'))\n",
    "# pickle.dump(test, open(DATA_TO_SAVE + \"_tmp_test_data.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# train = pickle.load(open(DATA_TO_SAVE + \"_tmp_train_data.pkl\", 'rb'))\n",
    "# test = pickle.load(open(DATA_TO_SAVE + \"_tmp_test_data.pkl\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_changes()\n",
    "test.reset_changes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Particionado inicial de los datos si fuera necesario </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.data_extraction import Data\n",
    "\n",
    "# X_train_Data, X_test_Data, y_train, y_test = all_data.train_test_split(\n",
    "#     criterion='windowed',\n",
    "#     train_size=.8,\n",
    "#     random_state=SEED,\n",
    "#     drop_columns=[]\n",
    "# )\n",
    "\n",
    "# X_train_Data = Data(X_train_Data)\n",
    "# X_test_Data = Data(X_test_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Preprocesamiento </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Eliminación de datos inválidos y valores atípicos </font>\n",
    "- TODO: Breve descripción de qué es un dato inválido (-1's en columna, etc.)\n",
    "- Eliminaremos aquellos valores fuera de los percentiles 5 y 95.\n",
    "- TODO: Definiremos cuál será el límite de outliers permitido por serie temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Remove invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.remove_outliers(\n",
    "    headers=train.get_derived_data_columns()['attrs'],\n",
    "    outliers_limit=.3\n",
    ")\n",
    "\n",
    "test.remove_outliers(\n",
    "    headers=test.get_derived_data_columns()['attrs'],\n",
    "    outliers_limit=.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remaining series\n",
    "print(\"Train: Previous number of series: {}\".format(\n",
    "    len(pd.unique(train.original_data['id']))))\n",
    "print(\"Train: Actual number of series: {}\".format(\n",
    "    len(pd.unique(train.derived_data['id']))))\n",
    "\n",
    "print(\"Test: Previous number of series: {}\".format(\n",
    "    len(pd.unique(test.original_data['id']))))\n",
    "print(\"Test: Actual number of series: {}\".format(\n",
    "    len(pd.unique(test.derived_data['id']))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Resoluciones a aplicar </font>\n",
    "- TODO:\n",
    "  - Si las series son rápidas (muchos cambios en poco tiempo) especificar resoluciones altas (sin modificaciones).\n",
    "  - Si las series son lentas (pocos cambios en mucho tiempo) especificar resoluciones bajas (eliminamos datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series lentas\n",
    "train.reduce_sampling_rate(remove_one_each_n_samples=2)\n",
    "test.reduce_sampling_rate(remove_one_each_n_samples=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Codificación </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Normalización </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.codifications import standardize_data\n",
    "\n",
    "train.derived_data, test.derived_data = standardize_data(\n",
    "    train.derived_data,\n",
    "    test.derived_data,\n",
    "    headers=train.get_derived_data_columns()['attrs']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Tensores entrada y salida de la red </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All lengths must be equal\n",
    "series_length = train.get_shortest_serie().shape[0]\n",
    "n_dims = len(train.get_derived_data_columns()['attrs'])\n",
    "\n",
    "(\n",
    "    train.get_shortest_serie().shape[0],\n",
    "    train.get_largest_serie().shape[0],\n",
    "    test.get_shortest_serie().shape[0],\n",
    "    test.get_largest_serie().shape[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This will determine the number of series of each split\n",
    "train_n_series = pd.unique(train.derived_data['id']).shape[0]\n",
    "test_n_series = pd.unique(test.derived_data['id']).shape[0]\n",
    "\n",
    "(train_n_series, test_n_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = train.transform_derived_data_into_X_y()\n",
    "X_test, _ = test.transform_derived_data_into_X_y()\n",
    "\n",
    "y_train = train.derived_data.groupby('id').first()['class'].to_numpy()\n",
    "y_test = test.derived_data.groupby('id').first()['class'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Técnicas de balanceo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#cd0000> Asignación de pesos a las clases </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = {0: class_weights[0], 1: class_weights[1],\n",
    "                 2: class_weights[2], 3: class_weights[3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classifier_utils import apply_lstm_format\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sequences_fragmenter = 2\n",
    "\n",
    "enc = LabelEncoder()\n",
    "enc.fit(y_train)\n",
    "\n",
    "X_train, y_train = apply_lstm_format(\n",
    "    X_train, y_train, train_n_series, series_length, sequences_fragmenter, enc)\n",
    "X_test, y_test = apply_lstm_format(\n",
    "    X_test, y_test, test_n_series, series_length, sequences_fragmenter, enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Diseño de la topología de red </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Preparación de mecanismo argmax en caso de salida multiclase </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def argmax(x, n_classes):\n",
    "    all_predictions = []\n",
    "    for max_class_position in K.argmax(x):\n",
    "        prediction = np.zeros(n_classes)\n",
    "        prediction.put(max_class_position, 1)\n",
    "        all_predictions.append(prediction)\n",
    "    return np.asarray(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "# Number of initial dimensions\n",
    "nn.add(LSTM(units=12, dropout=.2, recurrent_dropout=.2))\n",
    "# Number of Epilepsy's classes\n",
    "nn.add(Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Compilación de la red </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "metrics = [\n",
    "    k.metrics.CategoricalAccuracy(name=\"ACC\"),\n",
    "    k.metrics.Precision(name='Prec'),\n",
    "    k.metrics.Recall(name='Rec'),\n",
    "    k.metrics.AUC(name='AUC')\n",
    "]\n",
    "\n",
    "nn.compile(optimizer=RMSprop(\n",
    "    learning_rate=1e-4), loss='binary_crossentropy', metrics=metrics)\n",
    "nn.build(input_shape=X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Visualización de resultados preliminares </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_metrics(history):\n",
    "    for metric in history.history.keys():\n",
    "        if not metric.startswith('val_'):\n",
    "            plt.plot(history.history[metric], label=metric)\n",
    "            plt.plot(history.history[f'val_{metric}'], label=f'val_{metric}')\n",
    "            plt.title(metric)\n",
    "            plt.ylabel('')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(loc=\"upper left\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Entrenamiento del modelo </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = nn.fit(X_train, y_train, epochs=epochs,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 class_weight=class_weights,\n",
    "                 verbose=1)\n",
    "nn.summary()\n",
    "print('\\n\\n')\n",
    "\n",
    "y_pred = argmax(nn.predict(X_test), 4)\n",
    "\n",
    "show_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Clasificación </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = enc.inverse_transform(y_test)\n",
    "y_pred = enc.inverse_transform(y_pred)\n",
    "print(confusion_matrix(y_real, y_pred))\n",
    "print(classification_report(y_real, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Discusión de los resultados </font>\n",
    "- Dimensionalidad de entrada inicial -> TODO\n",
    "- Vamos a elegir **diferentes funciones de activación**, **divisiones en ventanas de cada serie del conjunto de datos** (valor ``sequence_fragmenter`` del apartado ``Tensores de entrada y salida de la red``), **diferentes funciones de pérdida**, **optimizadores**, **regularizadores y Dropouts** en caso de sobreajuste, **número de capas ocultas** y, finalmente, **diferentes unidades de neuronas por capa oculta** y **la tasa de aprendizaje** (**estas dos últimas a optimizar** mediante una búsqueda aleatoria de hiper-parámetros).\n",
    "\n",
    "### <font color=#cd0000> Funciones de activación de las capas ocultas </font>\n",
    "- TODO: (lo más común es lo siguiente) Dado que tratamos con una entrada de **valores continuos** sería interesante utilizar una **función de activación ReLU** para las capas ocultas junto con una **inicialización de pesos ReLU** (``He_uniform`` en Keras).\n",
    "\n",
    "### <font color=#cd0000> Función de pérdida y capa de salida </font>\n",
    "- TODO: Al enfrentarnos a un **problema de clasificación binaria** la capa de **salida contendrá una sola neurona** con una **función de activación sigmoidal** con un umbral de ``0.5`` para determinar si es de una clase u otra (si la activación supera el umbral será de clase ``normal`` y ``abnormal`` de lo contrario).\n",
    "- TODO: En cuanto a **la función de pérdida** y dado que el problema es de clasificación binaria usaremos **la función ``binary_crossentropy``** de Keras.\n",
    "\n",
    "### <font color=#cd0000> Tamaño de ventana y número de ventanas de cada serie del conjunto de datos </font>\n",
    "- \\#TODO: ¿Por qué dividirlo en varias ventanas proporciona una mejor precisión al clasificador?\n",
    "\n",
    "### <font color=#cd0000> Optimizadores y número de capas ocultas </font>\n",
    "- **Probaremos inicialmente con un número de neuronas en las capas ocultas igual al de la dimensión de entrada** y con **una tasa de aprendizaje brindada por la siguiente expresión: $\\frac{0.1}{nº\\_ejemplos\\_train}$**\n",
    "- En cuanto a los optimizadores probaremos el desempeño de los siguientes junto con diferentes arquitecturas de red:\n",
    "  - Adam:\n",
    "    - Red de 1 capa:\n",
    "    - Red de 2 capas:\n",
    "  - RMSprop\n",
    "    - Red de 1 capa:\n",
    "    - Red de 2 capas:\n",
    "  - SGD\n",
    "    - Red de 1 capa:\n",
    "    - Red de 2 capas:\n",
    "\n",
    "### <font color=#cd0000> Neuronas y tasa de aprendizaje </font>\n",
    "- **Para determinar cuáles son los rangos de neuronas y tasa de aprendizaje en los que buscar los mejores** escogeremos los mejores modelos encontrados hasta ahora y probaremos con diferentes codificaciones: (fan-in -> La mitad de neuronas que la capa anterior; fan-out -> el doble de neuronas que la capa anterior; tasa de aprendizaje base la hallada mediante la expresión: $\\frac{0.1}{nº\\_ejemplos\\_train}$)\n",
    "  - Fan-in + tasa aprendizaje más pequeña a la base\n",
    "  - Fan-out + tasa aprendizaje más pequeña a la base\n",
    "  - Fan-in + tasa aprendizaje más grande que la base\n",
    "  - Fan-out + tasa aprendizaje más grande que la base\n",
    "  - Fan-in + tasa aprendizaje regular\n",
    "  - Fan-out + tasa aprendizaje regular\n",
    "\n",
    "### <font color=#cd0000> Regularizadores y capa de Dropout </font>\n",
    "- **En caso de incurrir en sobreajuste** (algo que visualizaremos a posteriori) **probaremos a utilizar una capa de Dropout** con una tasa de no reajuste de pesos del ``0.2``, **de no conseguir resultados, utilizaremos regularizadores ``L1L2``** con ambos factores de regularización a ``0.01`` sobre todas las capas ocultas.\n",
    "- Tras realizar algunas pruebas observamos que hay sobreajuste y, para resolverlo, utilizaremos la capa de Dropout mencionada en el párrafo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Conclusiones </font>\n",
    "- Como podemos observar ...\n",
    "- No obstante si no tuviéramos más remedio que utilizarlo de esta forma buscaríamos el mejor en el rango orientativo:\n",
    "  - `n_units`: [...]\n",
    "  - `learning_rate`: [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Randomized Search </font>\n",
    "- Búsqueda de hiper-parámetros aleatoria con LSTM maximizando ``macro avg f1-score``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Rangos de búsqueda </font>\n",
    "- Como vimos anteriormente los rangos de búsqueda aleatoria de los mejores hiper-parámetros serán los siguientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_UNITS_RANGE = TODO\n",
    "LEARNING_RATE_CHOICES = TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import utils.constants as cs\n",
    "from utils.classifier_utils import (windowed_cross_val,\n",
    "                                    compute_classification_reports_means)\n",
    "from utils.plot_utils import pretty_print_classification_report_dict\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "def lstm_randomized_search_cv(\n",
    "        windowed_series,\n",
    "        relation_with_series,\n",
    "        prefix,\n",
    "        cv=5,\n",
    "        lstm_dict={}):\n",
    "    global PKL_DIR\n",
    "    all_clf_used = {}\n",
    "\n",
    "    n_samples = 5\n",
    "    units_list = random.sample(\n",
    "        list(N_UNITS_RANGE), n_samples)\n",
    "    learning_rate_list = random.sample(\n",
    "        LEARNING_RATE_CHOICES, n_samples)\n",
    "\n",
    "    best_hyp_params = None\n",
    "    best_score = 0\n",
    "    for units in units_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            lstm_dict[cs.LSTM_HYP_PARAM_UNITS] = units\n",
    "            lstm_dict[cs.LSTM_HYP_PARAM_LEARNING_RATE] = learning_rate\n",
    "            reports = windowed_cross_val(\n",
    "                None,\n",
    "                windowed_series,\n",
    "                relation_with_series,\n",
    "                cv=cv,\n",
    "                seed=SEED,\n",
    "                drop_columns=['class'],\n",
    "                estimator_type=cs.ESTIMATOR_LSTM,\n",
    "                lstm_dict=lstm_dict\n",
    "            )\n",
    "\n",
    "            mean_report = compute_classification_reports_means(reports)\n",
    "            all_clf_used[(units, learning_rate)] = mean_report\n",
    "\n",
    "            if mean_report['macro avg']['f1-score'][0] >= best_score:\n",
    "                best_score = mean_report['macro avg']['f1-score'][0]\n",
    "                best_hyp_params = (units, learning_rate)\n",
    "                best_report = mean_report\n",
    "\n",
    "            print(\"\\t\\t--------------ACTUAL BEST: Units={}; Learning Rate={}--------------\"\n",
    "                  .format(best_hyp_params[0], best_hyp_params[1]))\n",
    "            pretty_print_classification_report_dict(best_report)\n",
    "            print(\"\\t\\t--------------ITERATION: Units={}; Learning Rate={}--------------\"\n",
    "                  .format(units, learning_rate))\n",
    "            pretty_print_classification_report_dict(mean_report)\n",
    "\n",
    "    with open(PKL_DIR + prefix, 'wb') as file:\n",
    "        pickle.dump(all_clf_used, file)\n",
    "\n",
    "    return best_hyp_params, best_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE -> ALGUNOS DE LOS PARÁMETROS DEL MODELO SON FIJADOS EN EL MÉTODO lstm_build_model del módulo\n",
    "# classifier_utils.py ante problemas a la hora de clonar modelos neuronales de Keras.\n",
    "\n",
    "lstm_dict = {\n",
    "    cs.LSTM_SERIES_LENGTH: series_length,\n",
    "    cs.LSTM_SEQUENCES_FRAGMENTER: 2,\n",
    "    cs.LSTM_FITTED_LABELS_ENCODER: enc,\n",
    "    cs.LSTM_ARGMAX_FUNCTION: argmax,\n",
    "    cs.LSTM_N_CLASSES: 4,\n",
    "    cs.LSTM_CLASS_WEIGHTS: class_weights,\n",
    "    cs.LSTM_HYP_PARAM_EPOCHS: 50\n",
    "}\n",
    "\n",
    "lstm_randomized_search_cv(\n",
    "    train.derived_data,\n",
    "    train.derived_data_windows_per_serie,\n",
    "    PKL_NAME,\n",
    "    cv=5,\n",
    "    lstm_dict=lstm_dict\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Randomized Search con múltiples ejecuciones en lugar de Validación Cruzada </font>\n",
    "- Solo si tenemos pocos datos\n",
    "- Ejecutaremos el mismo modelo sobre diferentes particiones del conjunto de datos original para observar su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Análisis de resultados </font>\n",
    "- Según la búsqueda aleatoria de hiper-parámetros, la mejor combinación, es la de ``n_units`` = TODO y ``learning_rate`` = TODO:\n",
    "    ```\n",
    "    TODO\n",
    "    ```\n",
    "- Ahora vamos a visualizar la evolución de los resultados (25 resultados) para observar cómo avanza nuestra métrica objetivo -> Macro Average F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_reports = pickle.load(open(PKL_DIR + PKL_NAME, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_score\n",
    "\n",
    "macro_avg_f1_scores = dict(map(\n",
    "    lambda z: (z, {'score': all_reports[z]['macro avg']['f1-score'][0],\n",
    "                   'std': all_reports[z]['macro avg']['f1-score'][1]}),\n",
    "    all_reports\n",
    "))\n",
    "\n",
    "plot_score(\n",
    "    [macro_avg_f1_scores],\n",
    "    ('n_units', 'learning_rate'),\n",
    "    'LSTM',\n",
    "    inverse=False,\n",
    "    mode='score',\n",
    "    in_same_graphic=True,\n",
    "    accuracy_mode='accuracy',\n",
    "    metric_name='Macro Average F1-Score'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_recall_scores = dict(map(\n",
    "    lambda z: (z, {'score': all_reports[z]['abnormal']['recall'][0],\n",
    "                   'std': all_reports[z]['abnormal']['recall'][1]}),\n",
    "    all_reports\n",
    "))\n",
    "\n",
    "normal_recall_scores = dict(map(\n",
    "    lambda z: (z, {'score': all_reports[z]['normal']['recall'][0],\n",
    "                   'std': all_reports[z]['normal']['recall'][1]}),\n",
    "    all_reports\n",
    "))\n",
    "\n",
    "plot_score(\n",
    "    [abnormal_recall_scores],\n",
    "    ('n_units', 'learning_rate'),\n",
    "    'LSTM',\n",
    "    inverse=False,\n",
    "    mode='score',\n",
    "    in_same_graphic=True,\n",
    "    accuracy_mode='accuracy',\n",
    "    metric_name='Abnormal recall score'\n",
    ")\n",
    "\n",
    "plot_score(\n",
    "    [normal_recall_scores],\n",
    "    ('n_units', 'learning_rate'),\n",
    "    'LSTM',\n",
    "    inverse=False,\n",
    "    mode='score',\n",
    "    in_same_graphic=True,\n",
    "    accuracy_mode='accuracy',\n",
    "    metric_name='Normal recall score'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#cd0000> Evaluación sobre el conjunto de validación </font>\n",
    "- Vamos a llevar a cabo la evaluación final sobre el conjunto de validación (esto es lo que irá al apartado de ``Pruebas y Resultados`` de la memoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#cd0000> Construcción del modelo </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "# Number of initial dimensions\n",
    "nn.add(LSTM(units=12, dropout=.2, recurrent_dropout=.2))\n",
    "# Number of Epilepsy's classes\n",
    "nn.add(Dense(4, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "metrics = [\n",
    "    k.metrics.CategoricalAccuracy(name=\"ACC\"),\n",
    "    k.metrics.Precision(name='Prec'),\n",
    "    k.metrics.Recall(name='Rec'),\n",
    "    k.metrics.AUC(name='AUC')\n",
    "]\n",
    "\n",
    "nn.compile(optimizer=RMSprop(\n",
    "    learning_rate=1e-4), loss='binary_crossentropy', metrics=metrics)\n",
    "nn.build(input_shape=X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#cd0000> Entrenamiento </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = nn.fit(X_train, y_train, epochs=epochs,\n",
    "                 validation_data=(X_test, y_test))\n",
    "nn.summary()\n",
    "print('\\n\\n')\n",
    "\n",
    "y_pred = argmax(nn.predict(X_test), 4)\n",
    "\n",
    "show_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=#cd0000> Clasificación </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = enc.inverse_transform(y_test)\n",
    "y_pred = enc.inverse_transform(y_pred)\n",
    "print(confusion_matrix(y_real, y_pred))\n",
    "print(classification_report(y_real, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#cd0000> Conclusiones </font>\n",
    "- TODO - Unas breves conclusiones sobre los resultados obtenidos (influencia de la codificación, ...)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a096ff48e453ebcfb843df4ab333716150f391fd6f6f069a95d41746473af77b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tfg_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
